

### flume

> **Flume 是一个为“日志→大数据存储”量身打造的可扩展日志传输平台** ，在 Hadoop 生态中非常常见，适合高吞吐、批量、可扩展的日志处理需求。

假设你在 1000 台服务器上运行一个网站，每台机器都会产生日志文件。你可以在每台机器部署一个 Flume Agent：

1. 它读取本地日志文件；
2. 暂存在 Channel；
3. 定时将数据批量发送到 HDFS 或 Kafka。

这样你就不需要手动拉日志，而是实时地“推送”到中心系统，供大数据分析用。


Flume 的设计是“流水线式”的，基于三个核心组件组成：

| 组件              | 作用                                                              |
| ----------------- | ----------------------------------------------------------------- |
| **Source**  | 日志数据的入口，比如监听文件、接收 HTTP、Kafka 消息等             |
| **Channel** | 数据缓冲区（内存或文件），保证日志在传输过程中的暂存与可靠性      |
| **Sink**    | 数据输出端，负责把日志写入目标系统，如 HDFS、Kafka、ElasticSearch |

这些组件串成一个 “Agent”——部署在每台机器上，负责收集并转发本地日志。
