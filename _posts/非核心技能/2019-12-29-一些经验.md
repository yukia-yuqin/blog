---
layout: post
title: 一些经验
tags: interview c++
categories: interview
---

在所里学习的

1. 发音
    1. 名称音：字母在字母表中的读音
    2. 发音：字母在单词中的读音



### Magent
- [MAgent介绍]
- [githubcode]
- MAgent 是一个多 agent 增强学习研究平台，可同时支持数百甚至数百万的 agent 进行增强学习研究。

### pycharm远程连接服务器
- [pycharm远程连接服务器]

### ADK20E数据集
1. ADK20E
2. 灰度图像意思是RGB三个通道值一样的图像，一般读取一个通道的值，可以为8位，16位等，位数越小黑白越分明
3. 这个数据集的种类很多，有3148多种，不能用8位的读取
4. 这个数据集用R G 两个通道表示图的语义信息。


### 调参经验
- 	模型调参的时候主要有： crop_width  crop_heigth 大小，这里应该把两个设置为一样大，因为有ASPP空洞卷积？如果不一样大的话会卷入黑色的？？（这里详细了解一下）
- Learning_rate大小很重要，在fine_tune的时候通常从0.001开始，结束的时候根据迭代次数的增大而终止的lr要变小，通常为0.001/100, batch_size在有限的显存环境下越大越好，泛化性更好，我们在V100 32G显存下设的是 16的batch_size，我的lr开始是0.001，迭代次数是80，000，epocs = 80，000*16/20210 =64epocs，最终lr一开始设的是1e-6，但是发现epocs跑完之后发现指标仍然有上涨的趋势。于是调整lr和epocs继续训练。
-  遇到的问题：lr初值过大，使得学习不到图片中的细节。
- 评价标准：
	loss一直在降 
	cross_entropy 一直在降
	Pixle_Accuracy 0.6553 
	Mean_iou 18.59%, 官方 数据集 最高39%
	训练mean_iou 只比测试的大一点，测试基本和训练一致
- 	_NUM_CLASSES = 151
	_HEIGHT = 513
	_WIDTH = 513
	_DEPTH = 3
	_MIN_SCALE = 0.8
	_MAX_SCALE = 2.0
	_IGNORE_LABEL = 0
	_POWER = 0.9
	_MOMENTUM = 0.9
- 	为什么ARAP能量最小化函数能指导压缩?
	- 前提: 我们的压缩之后的网格是绝对不会比原始的网格大的.
	- 第二点：在a的前提下，每个网格的权重等于像素的权重的平均，全部像素都是特征的网格部分几乎约等于原始的图片的网格大小。其他的部分按照权重压缩大小。
-  'SSIM对比的结果
	- bitrate	0.12	0.17	0.29	0.47	0.67
	- psnr(dB)	27.9	28.4	29.8	32	33.5
    - ssim	0.765	0.78	0.84	0.89	0.925
-  Slim 需要从官方下载
- Inference 调通
```shell
    ffmpeg -i .\stuttgart_00_000000_%06d_leftImg8bit.png -vcodec mpeg4 test.avi
 ```
 - 视频缓冲块 VB ：
	公共缓冲池中有很多个缓冲池，每个缓冲池有很多个缓冲块(B1、B2、、、Bn)。进行视频编解码时，首先VI模块从VB中取得Bm缓冲块，处理后扔给VPSS，VPSS再根据需要（比如裁剪、缩放等）将Bm变换成Bi、Bj、Bk等缓冲块。这些缓冲块会被流向VENC、VDA、VO等模块，最后处理完成后，会释放这些缓冲块到缓冲池中，方便下一次使用。至于具体的流向，比如VPSS->VENC需要根据提供的API函数进行绑定。
    - 视频缓冲块VB是指视频处理的时候的单元，而chunk是视频发送的时候的单元

### 好的论文要求
- 正样本的“视觉”特点：
    1. 里面有几段公式，看上去文章显得似乎很专业，也显得作者似乎数学不错；
    2. 实验部分里面多少要有几个曲线图，即使那几个曲线图说明不了什么。但是，只要有几个曲线图在那里，起码表示我做的是“科学实验”；
    3. 最好在文章开头或者最后一页排列一堆图像。其实，我也注意到很多作者喜欢排列很多dataset里面的图像到paper上——即使那是一个 publically available的standard dataset——我不知道这样做的意义何在——除了审美效果。
    4. 最好写满8页，代表分量足够。

- 负样本的特点：
    1. 不够页数。在submission阶段，写不满6页的文章被录用的机会很小。虽然最后很多本来8页的文章还是能很神奇地被压缩到6页，如果作者想省掉 200美元的附加页费。题外话，我也一直不明白为什么多一页要多交100美元注册费。
    2. 有很大的数字表，就是m行n列，排满数字那种。这篇文章表明，排列了很多曲线图和柱状图的文章比排列了很多数字表的文章有更大概率被接收。
    3. 没有漂亮插图。

### 论文修改
- 1，总体感觉很仓促，只要细读发现很多问题，这个版本被录用概率不高，因为感觉不认真，这个是很麻烦的
- 2，题目不通，based用法不对
- 7，我始终认为Feature-preserving Image Retageting这种描述不正确，尤其Feature-preserving，不能正确描述我当时跟你们说的技术。你直接翻译基于特征的视频传输方法，都比现在准确Feature based video transmission
- 3，上次跟你说Fog需要大写，ROI需要大写，HEVC需要大写，你现在的写法很容易被当入门拒掉
- 4，对比算法，应写ROI中的一个特性，ROI技术已经20年了，这样很容易被误会
- 5，A. Related Terms这种描述不出现在论文中
- 6，定义描述很多不对，例如Mark the original image with height H and width W and cover a uniform grid over the
- 8，IV. FPVTRANS IMPLEMENT，这样题目很危险，不是技术论文，
- 9，A. Video Feature Extraction，很少见一个section只有文字
- 10，B. Video down-sampling，觉得写基于能量函数的最优特征选择，都比下采样好
- 11，条件（4）第一个，SSIM（），这种描述不对，你后面写e.g.有SSIM或PSNR，会被误会
- 12，The primal function is hard to solve类似这种描述错误，函数怎么能solve一般是problem，lagrangian第一字母大写
- 13，We need to calculate the optimal，通常不这样描述，你写作按自己想法，这样不对，应按第一次看你文章人，会怎样。写文章不是给自己看，我讲过很多次
- 14，算法中，不应出现2.12.2
- 15，EXPERIMENT， A. Dataset，D. Experiments on transmission让人感觉作者惜字如金，至少应认真写全
- 16，图太少，太小，感觉实验不充分，应多实验图表
- 17，实验描述，we set the constrain of SSIM (which is C in Formula. 3) as 这样写不对，例如公式（5），这样才对，你写的看着不全
- 18，通篇写we，例如contribution，写了3个we，这个看着怪怪的
- 19，In the future, we will focus on utilizing the redundancy between the frames to reduce more
- data and obtain less system delay.有错，很多这种错误，不能精读
- 20，[10] reference Jiang，这种好多处，应改正
- 21，写作不认真随处可见，例如C. transmission model，这个会影响review对文章录用。很多非语法错误检查时可以去掉的，写的好不好，认真是第一决定因素，所以类似这种错误应该去掉


### 基尼不纯度
- 基尼不纯度的大概意思是 一个随机事件变成它的对立事件的概率
- 例如 一个随机事件X ，P(X=0) = 0.5 ,P(X=1)=0.5 
- 那么基尼不纯度就为 P(X=0)(1 - P(X=0)) + P(X=1)(1 - P(X=1)) = 0.5
-  Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率
- 当为二元切分法时，Gini(P) = 2p(1-p)，它易于对树构建过程进行调整以处理连续性特征。
- 求基尼系数的含义：基尼系数宏观表达的是描述一个集合的混乱程度。基尼指数越大，表示这个样本集合D越混乱。这和熵的作用有些类似，但这仅仅是一个指数，能够判断大小，但不能像熵一样线性的量化集合中的混乱程度。

### 推荐系统
- FM
    - 原始方法：线性回归，太简单，特征独立，没有相互关联, FM加了特征工程的线性回归
    - 问题：参数空间过大,这里为$O(n^2)$，在处理互联网数据时，特征两级别可能是亿级别的；需要人工经验，这里一般会选择某些特征来组合，此时人工/专家经验就会很重要；样本量过滤稀疏，实际上那这种方式拿到的特征会是很稀疏的，对于在训练样本中未出现过的组合该模型无能为力

- DeepFM
    - 	深度学习本身的好处：理论上可以拟合任何函数，表征能力很强；无需特征工程（传统的用特征交叉）；处理非结构数据非常好；准确的学到Item和user的非线性表示（矩阵分解是线性的）。
    - 助FNN的思想，利用FM进行embedding，之后的wide和deep模型共享embedding之后的结果。DNN的输入完全和FNN相同（这里不用预训练，直接把embedding层看作一层的NN），而通过一定方式组合后，模型在wide上完全模拟出了FM的效果（至于为什么，论文中没有详细推导，本文会稍后给出推导过程），最后将DNN和FM的结果组合后激活输出。
    - 	而本文提出了DeepFM模型可以以端对端的方式来学习不同阶组合特征的模型，并且不需要其他特征工程。本文主要贡献点如下：本文提出了全新的神经网络模型，命名为DeepFM。DeepFM的结构中包含了因子分解机部分以及深度神经网络部分，分别负责低阶特征的提取和高阶特征的提取。与Wide&amp;Deep Model不同，DeepFM在训练过程中不需要特征工程。


### 基础算法
- 数据挖掘十大算法
- [最大流]

### 概率论
- 基本定义   随机变量独立  概率密度  期望/均值与方差  协方差和相关系数   概率分布模型    两个变量连乘取log等于求和里面再求和   一、数学期望和方差  二、正太分布  三、矩阵求导  四、相关矩阵  五、正交矩阵  六、共轭矩阵
- 数学现象
    - 曼德勃罗集合
- 常用数学方法
    - 奇异值分解  数学优化方法  凸二次规划    Em算法
- 最优化计算方法
    1. 最优化问题概述 
    *介绍最优化问题分类以及求解思路
    2. 线搜索方法 
    *基于线搜索方法，包括最速下降、牛顿方法以及步长计算等
    3. 信赖域方法 
    *介绍信赖域求解最优化问题的思路
    4. 共轭梯度方法 
    *介绍共轭方法的思路
    5. 拟牛顿方法 
    *介绍拟牛顿方法，用一阶梯度近似Hessian矩阵方法
    6. 大规模无约束最优化方法 
    *大规模无约束问题，LBFGS等
    7. 梯度计算 
    *复杂函数梯度近似方法
    8. 无梯度最优化方法 
    *不计算梯度情况下，如何进行最优化
    9. 最小二乘问题 
    *最优化方法应用，求解最小二乘问题
    10. 非线性方程 
    *最优化方法应用，求解非线性方程问题
    11. 有约束最优化问题 
    *介绍等式、非等式约束最优化问题以及最优化条件，包括KKT条件、对偶等
    12. 线性规划问题 
    *线性规划常见求解算法
    13. 非线性约束最优化问题 
    *介绍非线性约束的最优化问题求解思路
    14. 二次规划问题 
    *目标函数是二次函数的特殊最优化问题，是SQP、内点等方法的基础
    15. 惩罚和增广拉格朗日方法 
    *求解带约束最优化问题常用方法
    16. 序列二次规划和内点法 
    *SQP和IP方法对于求解大规模约束最优化问题提供方案

### yukia
- Y is for young, the years never show! 
- U isfor unique, your love of life. 
- K is for knowledge, an avid learner 
- I is for interest. you show in others. 
- A is for amenable, for your easy going nature. 

### 搜索
- intitle weather filetype：PDF inurl:pdf











 [最大流]:https://github.com/JanRocketMan/DinitzAlgorithm
[MAgent介绍]:https://baijiahao.baidu.com/s?id=1586016559621774215&wfr=spider&for=pc&isFailFlag=1
[githubcode]:https://github.com/geek-ai/MAgent
[pycharm远程连接服务器]:https://blog.csdn.net/zhaihaifei/article/details/53691873