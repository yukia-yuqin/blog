## 写爬虫爬取github spring issue

```
import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import random
import time
from translate import Translator

# 创建 Translator 对象，指定翻译源语言和目标语言
translator = Translator(from_lang='en', to_lang='zh')

def find_parse_bug(i, bug_data_list):
    base_url = "https://github.com/spring-projects/spring-framework/releases?page=" + str(i)
    response = requests.get(base_url)
    soup = BeautifulSoup(response.text, 'html.parser')

    release_urls = []
    sections = soup.find_all('section')
    for section in sections:
        version = section.find('h2', class_='sr-only', id=lambda x: x and x.startswith('hd-'))
        bug_fixes_h2 = section.find('h2', string=re.compile(r'bug fix', flags=re.IGNORECASE))
        if bug_fixes_h2:
            # 找到 <h2> 元素后的下一个 <ul> 元素
            bug_fixes_ul = bug_fixes_h2.find_next('ul')

            if bug_fixes_ul:
                # 遍历 <ul> 中的所有 <li> 元素
                bug_items = bug_fixes_ul.find_all('li')

                for bug_item in bug_items:
                    bug_data = {}
                    bug_data['version'] = version.text
                    # 提取 <li> 中的描述
                    summary = bug_item.text.strip()
                    bug_data['summary'] = summary
                    try :
                        bug_data['chinese_summary'] = translator.translate(summary)
                    except:
                        bug_data['chinese_summary'] = ""
                        pass

                    # 找到链接 <a> 元素，并获取 href 属性
                    link_element = bug_item.find('a')
                    if link_element:
                        link = link_element['href']
                        # 访问链接并获取内容
                        response = requests.get(link)
                        bug_content_soup = BeautifulSoup(response.text, 'html.parser')
                        # 解析其中的label
                        parse_bug_label(bug_data, bug_content_soup)
                        # 解析其中的描述
                        parse_bug_desp(bug_data, bug_content_soup)

                        bug_data_list.append(bug_data)
                        # 生成一个随机停顿时间，范围在1到3之间
                        random_sleep_time = random.uniform(1, 3)

                        # 执行随机停顿
                        time.sleep(random_sleep_time)

                    print("-" * 50)
            else:
                print("No <ul> found after Bug Fixes.")
        else:
            print("Bug Fixes section not found.")

    return release_urls


def parse_bug_label(bug_data, bug_content_soup):
    bug_label_div = bug_content_soup.find('span', string='Labels')

    bug_label_parent_div = bug_label_div.find_parent('div', class_='d-flex mb-3')
    if bug_label_parent_div:
        # 在 <div> 下找到所有的 <span> 元素
        span_elements = bug_label_parent_div.find_all('span')
        # 将每个 <span> 中的内容串成字符串
        span_contents = [span_element.text.strip() for span_element in span_elements]
        # 将列表中的字符串连接成一个字符串
        bug_data['label'] = ' '.join(span_contents)


def parse_bug_desp(bug_data, bug_content_soup):
    # 找到第一个包含类名 user-select-contain 的 table 元素
    description_table = bug_content_soup.find('table', class_='user-select-contain')
    if description_table:
        # 找到第一个 td 元素
        td_element = description_table.find('td')

        if td_element:
            # 提取第一个 td 中所有 <p> 和 <div> 标签中的内容
            content_list = []
            chinese_content_list = []

            for tag in td_element.find_all(['p', 'div']):
                tag_text = tag.get_text(strip=True)
                # 如果标签是 <p>，则进行翻译
                if tag.name == 'p':
                    chinese_tag_text = translator.translate(tag_text)
                else:
                    chinese_tag_text = tag_text
                content_list.append(tag_text)
                chinese_content_list.append(chinese_tag_text)

            # 使用换行符拼接内容
            bug_data['description'] = '\n'.join(content_list)
            bug_data['chinese_description'] = '\n'.join(chinese_content_list)
    else:
        print("No table element found with class 'user-select-contain'.")


if __name__ == "__main__":
    for i in (1, 2):
        bug_data_list = []
        find_parse_bug(i, bug_data_list)
        # 将数据写入 Excel 文件，如果文件不存在则创建，如果存在则追加
        excel_file_path = 'springBug.xlsx'
        with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='a') as writer:
            # 将数据转换为 DataFrame 并写入 Excel
            df = pd.DataFrame(bug_data_list)
            df.to_excel(writer, index=False, header=not pd.io.excel._is_excel_file_empty(excel_file_path))

```
